I am using Deep Q-learning for the project.

I am getting a scalar value for the`action` when I try to calculate from `argmax(Qs)`.

The relevant part of the main .py file (equivalent to the notebook) is:

>     for ep in range(1, train_episodes):
>         total_reward = 0
>         t = 0
>         while t < max_steps:
>             step += 1
> 
>             # Explore or Exploit
>             explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * step)
>             if explore_p > np.random.rand():
>                 # Make a random action
>                 action = agent.action_sample()
>             else:
>                 # Get action from Q-network
>                 feed = {mainQN.inputs_: state.reshape((1, *state.shape))}
>                 Qs = sess.run(mainQN.output, feed_dict=feed)
>                 action = np.argmax(Qs)
> 
>             # Take action, get new state and reward
>             next_state, reward, done = agent.task.step(action)

The relevant part of the agent.py file is:

>         with tf.variable_scope(name):
>             self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')
> 
>             # One hot encode the actions to later choose the Q-value for the action
>             #self.actions_ = tf.placeholder(tf.int32, [None], name='actions')
>             #one_hot_actions = tf.one_hot(self.actions_, action_size)
>             self.actions_ = tf.placeholder(tf.float32, [None, action_size], name='actions')
> 
>             # Target Q values for training
>             self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')
> 
>             # ReLU hidden layers
>             self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)
>             self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)
> 
>             # Linear output layer
>             self.output = tf.contrib.layers.fully_connected(self.fc2, action_size,
>                                                             activation_fn=None)
> 
>             ### Train with loss (targetQ - Q)^2
>             # output has length 2, for two actions. This next line chooses
>             # one value from output (per row) according to the one-hot encoded actions.
>             #self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)
>             self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)

Both files main.py and agent.py are located at the main folder: https://github.com/suvoganguli/DeepLearning/tree/master/Part6_ReinforcedLearning/Project-Quadcopter
with the commit message `In progress`.

Can you please take look at it? I have spent quite a while looking at it but can't figure what I am getting an error.

Thanks.